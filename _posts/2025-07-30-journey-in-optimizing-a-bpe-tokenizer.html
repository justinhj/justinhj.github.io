---
layout: post
title: Let's build the GPT Tokenizer in C++
tags: [ai, cpp, c++, python, optimization, performance, llm, tokenizer]
---
<link rel="stylesheet" type="text/css" href="../../../_orgcss/site.css" />

<div id="outline-container-org2e0801c" class="outline-2">
<h2 id="org2e0801c">Introduction</h2>
<div class="outline-text-2" id="text-org2e0801c">
<p>
In February 2024, reknowned AI researcher Andrej Karpathy published the following video. 
</p>

<p>
<a href="https://www.youtube.com/watch?v=zduSFxRajkE">https://www.youtube.com/watch?v=zduSFxRajkE</a>
</p>

<p>
An excellent introduction to the fascinating topic of tokenization,
something he considers a necessary evil right now. Maybe the need for
it will go away but for now it must be well understood by AI engineers
in order to get good results when building and using large language
models.
</p>

<p>
As a software engineer in 2025 LLM and generative AI in general brings
a profound shift into how I design, develop and implement systems. As
such I have been delving into the details of how they they work with
Tokenization being a significant side quest.
</p>

<p>
In this blog, related video and accompanying C++ project, I will walk
through my exploration of the training aspect of tokenizers and look
at their performance characteristics and optimization opportunities.
</p>
</div>
</div>

<div id="outline-container-org5a48473" class="outline-2">
<h2 id="org5a48473">Briefly, Tokenization and BPE</h2>
<div class="outline-text-2" id="text-org5a48473">
<p>
I don't want to even try and do a better job of explaining the need
for tokenization and the BPE algorithm, so I instead recommend
watching Karpathy's video and for additional insights read the paper
below:
</p>

<p>
[Neural Machine Translation of Rare Words with Subword Units](<a href="https://arxiv.org/pdf/1508.07909">https://arxiv.org/pdf/1508.07909</a>)
</p>
</div>

<div id="outline-container-org457d3c1" class="outline-3">
<h3 id="org457d3c1">The need for tokenization</h3>
<div class="outline-text-3" id="text-org457d3c1">
<ul class="org-ul">
<li>Large language models deal with numbers not words or characters, so we must map input text to numbers.</li>
<li>The model requires a fixed size vocabulary.</li>
<li>Unforunately that vocabulary would have to be enormous to fit all words from all languages.</li>
</ul>
</div>
</div>

<div id="outline-container-org61241d9" class="outline-3">
<h3 id="org61241d9">Sub word unit tokenization</h3>
<div class="outline-text-3" id="text-org61241d9">
<p>
The solution to these issues is sub-word tokenization; splitting words
up into numbers representing components of words. Once you have that
you can handle:
</p>

<ul class="org-ul">
<li>Open-Vocabulary: Part of the token set is the basic characters of each language so you can represent every word.</li>
<li>Rare words: Because the vocabulary set is open it means any rare word is handled.</li>
<li>Enables Translation of Novel Words: The model can translate and generate words it has not encountered before by composing them from sub-word units.</li>
</ul>

<p>
Tokenization means taking text and splitting it into subword units. An input text like <code>"My cat, Blivarian, is making a mess."</code> may be tokenized into something like this:
</p>

<p>
You can explore this tokenization here:
<a href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer</a>
</p>

<p style="text-xl">
  <span style="background-color: #d1c4e9;">My </span>
  <span style="background-color: #c8e6c9;">cat</span>
  <span style="background-color: #f0f4c3;">, </span>
  <span style="background-color: #ffcdd2;">Bl</span>
  <span style="background-color: #b3e5fc;">iv</span>
  <span style="background-color: #d1c4e9;">arian</span>
  <span style="background-color: #f0f4c3;">, </span>
  <span style="background-color: #c8e6c9;"> is</span>
  <span style="background-color: #ffcdd2;"> making</span>
  <span style="background-color: #b3e5fc;"> a</span>
  <span style="background-color: #d1c4e9;"> mess</span>
  <span style="background-color: #c8e6c9;">.</span>
</p>

<p>
[5444, 9059, 11, 3130, 569, 21203, 11, 382, 4137, 261, 13017, 13]
</p>

<p>
Notice that the commas have the same token value when appearing in
different places. Also that common words like cat and mess have their
own tokens.
</p>

<p>
I deliberately made up a name for the made up Cat that is not a real
word "Blivarian". You can see that it is split up into 3 sub
words. Without tokenization this would instead have been stored with a
special "Out of vocabulary" token, that means it carries no semantic
meaning. When dealing with sub word tokens however, the LLM has the
opportunity to build up meaning for those components that may help
with overall model quality.
</p>
</div>
</div>

<div id="outline-container-org5b03015" class="outline-3">
<h3 id="org5b03015">Byte Pair Encoding - BPE</h3>
<div class="outline-text-3" id="text-org5b03015">
</div>
<div id="outline-container-org6a0c0da" class="outline-4">
<h4 id="org6a0c0da">Why BPE?</h4>
<div class="outline-text-4" id="text-org6a0c0da">
<p>
From above we understand that we should split words into sub-word components to handle the vast space of human vocabulary in the finite space of the LLMs vocabulary.
</p>

<p>
How to do that is the next question. Why not, for example, just have a vocabulary consisting of the punctuation and alphabetic characters of every language?
</p>

<p>
It won't work well because in the LLM training it will build up an
embedding vector for each token, the unit of vocabulary. This vector
is an array of numbers that represents a direction in multidimensional
space. To us those numbers mean nothing, but in LLM training those
numbers, when used in conjunction with the rest of the models weights,
can be used to learn and represent all kinds of meaning.
</p>

<p>
Models like Transformers have a finite context window. When sequences
are excessively long, it becomes much harder for the model to capture
long-range dependencies and relationships between words that are far
apart. The model has to work harder to understand the overall context.
</p>

<p>
Instead we want something that splits things into meaningful chunks, morphemes, as well as capturing commonly used words with tokens. This ends up looking something like Huffman Encoding:
</p>

<p>
<a href="https://en.wikipedia.org/wiki/Huffman_coding">https://en.wikipedia.org/wiki/Huffman_coding</a>
</p>

<p>
It represents more frequently occuring substrings with less bits, giving us a more efficicent data size.
</p>

<p>
Similarly, BPE, is data-driven algorithm that creates a vocabulary of meaningful and frequently occurring subword units.
</p>
</div>
</div>

<div id="outline-container-orgcab669d" class="outline-4">
<h4 id="orgcab669d">BPE algorithm</h4>
<div class="outline-text-4" id="text-orgcab669d">
<p>
First you need to train across a large corpus of realistic text. For
state of the art (SOTA) LLMs this is likely in the trillions of
characters of data.
</p>

<p>
The algorithm itself is very simple, it works as follows:
</p>

<p>
Start with 256 tokens (0 to 255), our basic cgaracter set.
</p>

<ol class="org-ol">
<li>First turn the text into its underlying numberic representation (utf-8 format input as bytes is fine).</li>
<li>Count all the pairs of bytes.</li>
<li>Pick the most frequently occuring pair and generate the next new token (257, 258&#x2026;).</li>
<li>Replace that pair whereever it occurs with the new token.</li>
</ol>

<p>
Repeat until you have your full vocabulary. You can then save the merge pairs and these are then used by end users to encode their text before sending to the model. 
</p>

<p>
They can also be used to reconstuct the original text in the decoding process when the response comes from the model. 
</p>
</div>
</div>

<div id="outline-container-org60512a5" class="outline-4">
<h4 id="org60512a5">Conflict Resolution</h4>
<div class="outline-text-4" id="text-org60512a5">
<p>
An important decision in tokenization is how to handle pairs with the same frequency. In this post I'll consider two methods:
</p>

<ul class="org-ul">
<li>First in corpus wins.</li>
<li>Lexicographical ordering.</li>
</ul>

<p>
With any tokenization algorithm design we need to consider efficiency of implementation alongside methods that give the best results.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org241a45b" class="outline-2">
<h2 id="org241a45b">minbpe-cc. A rewrite of minbpe in modern C++.</h2>
<div class="outline-text-2" id="text-org241a45b">
<p>
It's a fair question. The Python code is clean, educational, and works perfectly. My motivation was twofold:
</p>

<ol class="org-ol">
<li><b>Self-Education</b>: The best way to learn something is to build it. Rewriting the code in a lower-level language like C++ would force me to confront every detail of the Byte-Pair Encoding (BPE) algorithm, from memory management to performance trade-offs.</li>
<li><b>The Craft of Optimization</b>: This is a classic software engineering exercise. Python is fantastic for prototyping and research, but for raw, number-crunching performance, we often turn to languages like C++. This project was a perfect real-world test case for that practice.</li>
</ol>
</div>
</div>

<div id="outline-container-org375050c" class="outline-2">
<h2 id="org375050c">The Heart of the Algorithm: Finding Frequent Pairs</h2>
<div class="outline-text-2" id="text-org375050c">
<p>
The core loop of the BPE algorithm is conceptually simple: find the most frequent pair of adjacent tokens in the text and merge them into a new, single token. You repeat this process for a set number of merges to build your vocabulary.
</p>

<p>
The interesting part is <i>how</i> you define "most frequent" when there are ties. This led to two different approaches in my C++ version.
</p>
</div>

<div id="outline-container-org5ac40fe" class="outline-3">
<h3 id="org5ac40fe">The First Pair (Compatible Mode)</h3>
<div class="outline-text-3" id="text-org5ac40fe">
<p>
To maintain 1:1 compatibility with Karpathy's original output, the first approach is simple: if multiple pairs have the same highest frequency, you pick the one that appears <b>first</b> in the current vocabulary list. This is straightforward but requires more searching.
</p>
</div>
</div>

<div id="outline-container-org27d07f9" class="outline-3">
<h3 id="org27d07f9">The Lexical Pair (Optimized Mode)</h3>
<div class="outline-text-3" id="text-org27d07f9">
<p>
For maximum speed, I implemented a second strategy. When multiple pairs have the same highest frequency, you instead pick the pair that comes first <b>lexicographically</b>. This small change allows for significant optimization in the search process, as we'll see in the benchmarks.
</p>
</div>
</div>
</div>

<div id="outline-container-org48c37b8" class="outline-2">
<h2 id="org48c37b8">The Payoff: Performance Benchmarks</h2>
<div class="outline-text-2" id="text-org48c37b8">
<p>
This is where the effort really shines. I ran the original Python code against my two C++ versions on two different datasets: the "Complete Works of Shakespeare" and a slice of the much larger "Wikitext-103" dataset.
</p>

<p>
The results are dramatic.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Tokenizer training time in seconds (lower is better).</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Implementation</th>
<th scope="col" class="org-left">Dataset</th>
<th scope="col" class="org-right">Time (seconds)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Python</td>
<td class="org-left">Shakespeare</td>
<td class="org-right">15.5</td>
</tr>

<tr>
<td class="org-left">C++ (First Pair)</td>
<td class="org-left">Shakespeare</td>
<td class="org-right">0.45</td>
</tr>

<tr>
<td class="org-left">C++ (Lexical Pair)</td>
<td class="org-left">Shakespeare</td>
<td class="org-right"><b>0.12</b></td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">Python</td>
<td class="org-left">Wikitext-103</td>
<td class="org-right">180.2</td>
</tr>

<tr>
<td class="org-left">C++ (First Pair)</td>
<td class="org-left">Wikitext-103</td>
<td class="org-right">5.1</td>
</tr>

<tr>
<td class="org-left">C++ (Lexical Pair)</td>
<td class="org-left">Wikitext-103</td>
<td class="org-right"><b>1.4</b></td>
</tr>
</tbody>
</table>

<p>
The C++ versions are orders of magnitude faster. The optimized "lexical pair" approach provides another 3-4x speedup over the already fast "compatible" C++ version.
</p>
</div>
</div>

<div id="outline-container-orge6dd077" class="outline-2">
<h2 id="orge6dd077">The C++ Struggle is Real</h2>
<div class="outline-text-2" id="text-orge6dd077">
<p>
Of course, these performance gains didn't come for free. Writing modern C++ can be a journey, and I hit a few notable roadblocks.
</p>
</div>

<div id="outline-container-org57f91ef" class="outline-3">
<h3 id="org57f91ef">Build Systems: CMake vs. Zig</h3>
<div class="outline-text-3" id="text-org57f91ef">
<p>
I started with CMake, the de-facto standard for C++ projects. However, I quickly found myself wrestling with boilerplate. I eventually discovered that I could use the <a href="https://ziglang.org/">Zig</a> toolchain not just for Zig code, but as an incredibly clean and simple C++ build system.
</p>

<p>
Compare the verbosity of a typical <code>CMakeLists.txt</code> to my final <code>build.zig</code> file:
</p>
<div class="org-src-container">
<pre class="src src-zig">const std = @import("std");

pub fn build(b: *std.Build) void {
    const target = b.standardTargetOptions(.{});
    const optimize = b.standardOptimizeOption(.{});

    const exe = b.addExecutable(.{
        .name = "minbpe",
        .root_source_file = .{ .path = "main.cpp" },
        .target = target,
        .optimize = optimize,
    });
    // ... link libraries
    b.installArtifact(exe);
}
</pre>
</div>

<p>
This was a revelation. It's cross-platform and ridiculously easy to use.
</p>
</div>
</div>

<div id="outline-container-orga7c9e4e" class="outline-3">
<h3 id="orga7c9e4e">The Regex Nightmare</h3>
<div class="outline-text-3" id="text-orga7c9e4e">
<p>
The GPT-4 tokenizer pattern uses a tricky bit of regex: a <b>negative lookahead</b>. Finding a modern, header-only C++ regex library with good Unicode support that could handle this correctly was surprisingly difficult.
</p>
</div>
</div>

<div id="outline-container-org48d1c0f" class="outline-3">
<h3 id="org48d1c0f">A Subtle Bug: Integer Sizes</h3>
<div class="outline-text-3" id="text-org48d1c0f">
<p>
I originally used a standard C++ <code>int</code> for token IDs. On my 64-bit system, this was overkill and could mask potential bugs. A <code>uint16_t</code> (max value 65,535) is a common choice, but to support vocabularies up to 100k, I settled on <code>uint32_t</code>. It's a small detail, but it's the kind of memory and type consideration that is front-and-center in C++ development.
</p>
</div>
</div>
</div>

<div id="outline-container-orge16fdc5" class="outline-2">
<h2 id="orge16fdc5">Conclusion</h2>
<div class="outline-text-2" id="text-orge16fdc5">
<p>
This project was a fantastic learning experience. It solidified my understanding of tokenization and was a practical lesson in the art of performance optimization. It demonstrates the enduring value of C++ for high-performance computing and highlights how modern tools like Zig can make the development experience much more pleasant.
</p>

<p>
If you want to dive into the code or run the benchmarks yourself, you can find the full project on GitHub.
</p>

<ul class="org-ul">
<li><a href="https://github.com/justinhj/minbpe-cc">https://github.com/justinhj/minbpe-cc</a></li>
</ul>

<p>
Thanks for reading!
</p>

<p>
&copy;2025 Justin Heyes-Jones. All Rights Reserved
</p>
</div>
</div>
