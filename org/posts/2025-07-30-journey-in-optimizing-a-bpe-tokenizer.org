#+TITLE: Let's build the GPT Tokenizer in C++
#+AUTHOR: Justin Heyes-Jones
#+DATE: 2025-07-30
#+STARTUP: showall
#+OPTIONS: toc:nil
#+CREATOR: <a href="https://www.gnu.org/software/emacs/">Emacs</a> 29.1 (<a href="http://orgmode.org">Org</a> mode 9.6)
#+BEGIN_EXPORT html
---
layout: post
title: Let's build the GPT Tokenizer in C++
tags: [ai, cpp, c++, python, optimization, performance, llm, tokenizer]
---
<link rel="stylesheet" type="text/css" href="../../../_orgcss/site.css" />
#+END_EXPORT

** Introduction

In February 2024, reknowned AI researcher Andrej Karpathy published the following video. 

https://www.youtube.com/watch?v=zduSFxRajkE

An excellent introduction to the fascinating topic of tokenization,
something he considers a necessary evil right now. Maybe the need for
it will go away but for now it must be well understood by AI engineers
in order to get good results when building and using large language
models.

As software engineers in 2025 the advent of large language models, and
generative AI in general, bring about a profound shift in how we
design, develop and implement software systems. As such I have been
delving into the details of how they they work with Tokenization being
a significant side quest.

In this blog, related video and accompanying C++ project, I will walk
through my exploration of the training aspect of tokenizers and look
at their performance characteristics and optimization opportunities.

** Briefly, Tokenization and BPE

I don't want to even try and do a better job of explaining the need
for tokenization and the BPE algorithm than Karpathy, so I instead
recommend watching his video and, for additional insights, read the
paper below:

[Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909)

*** The need for tokenization

- Large language models deal with numbers not words or characters, so we must map input text to numbers.
- The model requires a fixed size vocabulary.
- Unforunately that vocabulary would have to be enormous to fit all words from all languages.

*** Sub word unit tokenization

The solution to these issues is sub-word tokenization; splitting words
up into numbers representing components of words. Once you have that
you can handle:

- Open-Vocabulary: Part of the token set is the basic characters of each language so you can represent every word.
- Rare words: Because the vocabulary set is open it means any rare word is handled.
- Enables Translation of Novel Words: The model can translate and generate words it has not encountered before by composing them from sub-word units.

Tokenization means taking text and splitting it into subword units. An input text like ~"My cat, Blivarian, is making a mess."~ may be tokenized into something like this:

You can explore this tokenization here:
https://platform.openai.com/tokenizer

#+BEGIN_EXPORT html
<p style="text-xl">
  <span style="background-color: #d1c4e9;">My </span>
  <span style="background-color: #c8e6c9;">cat</span>
  <span style="background-color: #f0f4c3;">, </span>
  <span style="background-color: #ffcdd2;">Bl</span>
  <span style="background-color: #b3e5fc;">iv</span>
  <span style="background-color: #d1c4e9;">arian</span>
  <span style="background-color: #f0f4c3;">, </span>
  <span style="background-color: #c8e6c9;"> is</span>
  <span style="background-color: #ffcdd2;"> making</span>
  <span style="background-color: #b3e5fc;"> a</span>
  <span style="background-color: #d1c4e9;"> mess</span>
  <span style="background-color: #c8e6c9;">.</span>
</p>
#+END_EXPORT

[5444, 9059, 11, 3130, 569, 21203, 11, 382, 4137, 261, 13017, 13]

Notice that the commas have the same token value when appearing in
different places. Also that common words like cat and mess have their
own tokens.

I deliberately made up a name for the made up Cat that is not a real
word "Blivarian". You can see that it is split up into 3 sub
words. Without tokenization this would instead have been stored with a
special "Out of vocabulary" token, that means it carries no semantic
meaning. When dealing with sub word tokens however, the LLM has the
opportunity to build up meaning for those components that may help
with overall model quality.

*** Byte Pair Encoding - BPE

**** Why BPE?

From above we understand that we should split words into sub-word
components to handle the vast space of human vocabulary in the finite
space of the LLMs vocabulary.

How to do that is the next question. Why not, for example, just have a
vocabulary consisting of the punctuation and alphabetic characters of
every language?

It won't work well because in the LLM training it will build up an
embedding vector for each token, the unit of vocabulary. This vector
is an array of numbers that represents a direction in multidimensional
space. To us those numbers mean nothing, but in LLM training those
numbers, when used in conjunction with the rest of the models weights,
can be used to learn and represent all kinds of meaning.

Models like Transformers have a finite context window. When sequences
are excessively long, it becomes much harder for the model to capture
long-range dependencies and relationships between words that are far
apart. The model has to work harder to understand the overall context.

Instead we want something that splits things into meaningful chunks, morphemes, as well as capturing commonly used words with tokens. This ends up looking something like Huffman Encoding:

https://en.wikipedia.org/wiki/Huffman_coding

It represents more frequently occuring substrings with less bits, giving us a more efficicent data size.

Similarly, BPE, is data-driven algorithm that creates a vocabulary of meaningful and frequently occurring subword units.

**** BPE algorithm

First you need to train across a large corpus of realistic text. For
state of the art (SOTA) LLMs this is likely in the trillions of
characters of data.

The algorithm itself is very simple, it works as follows:

Start with 256 tokens (0 to 255), our basic character set.

1. First turn the text into its underlying numeric representation (typically just the bytes of a UTF-8 input).
2. Count all the pairs of bytes. 
3. Pick the most frequently occuring pair and generate the next new token (257, 258...).
4. Replace that pair whereever it occurs with the new token.

Repeat until you have your full vocabulary. You can then save the
merge pairs and these are then used by end users to encode their text
before sending to the model.

They can also be used to reconstuct the original text in the decoding
process when the response comes from the model.

**** Conflict Resolution

An important decision in tokenization is how to handle pairs with the same frequency. In this post I'll consider two methods:

- First in corpus wins. 
- Lexicographical ordering.

With any tokenization algorithm design we need to consider efficiency
of implementation alongside methods that give the best results. Some
of these concerns will be highlighted below.

"With these algorithmic decisions in mind, I was ready to dive into
the C++ implementation and see how they performed in practice. This
led to my project, minbpe-cc."

** minbpe-cc an exercise in optimization

For me I find the best way to learn a topic is to get my hands dirty,
and as such I decided to reimplement Karpathy's Python code in C++.

I also wanted to focus on optimization of the training stage, for no
other reason that curiousity.

Why C++?

- It's a low level language with generally low to zero cost abstractions.
- I've recently been catching up with modern C++ and wanted to try out some of the new features (C++23 required).

The final code here fully implements all the facets of Karpathy's minbpe including encoding, decoding and training. I've included end to end tests and tested in a linux and MacOS environment. I have not tested on Windows yet, but I expect it will work without much modification.

https://github.com/justinhj/minbpe-cc

** Implementation tales

Converting from Python to C++ is fairly straightforward although I hit some speed bumps on the way:

1. Regex compatibility.
2. Python dictionary behaviour. 
3. CMake. 

*** Regex compatibility

Firstly, what are regex's needed for? 

In the GPT series of tokenizers, OpenAI realized that is beneficial to
try and keep parts of text together, as such rather than run BPE on
the whole input text, they first divide it up into sections by the
following regular expressions:

* GPT2 ~"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+"~
* GPT4 ~"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+"~

These expressions are designed to preserve various aspects of English text rather than allow them to be split up during the merge process.

Whilst there are a few established regex libraries for C++ (writing my own being out of scope for this project), finding one that was capable of handling these regular expressions took some looking. 

These expressions need support for unicode matchers and also negative lookahead.

I compared several libraries:

1. RE2 from Google.
2. std::regex in the C++ standard library.
3. Boost::regex
4. Re-Flex

None of these met the requirements. 

In the end I found the Perl compatible PRE2 library worked the best.

The biggest footgun was that the Boost::regex library was asserting
because Boost was not linking properly with the ICU
(internationalization) library. I suspect this could be made to work
but I gave up.

*** Optimization mantras

In System's Performance, Enterprise and the Cloud by Brendan Gregg (2021) the following mantras for performance are listed, ordered from most to least effective. I find these useful when considering optimization.

1. Don’t do it.
2. Do it, but don’t do it again.
3. Do it less.
4. Do it later.
5. Do it when they’re not looking.
6. Do it concurrently.
7. Do it more cheaply.

We can refer to these during the post.

*** Data structures

The first step to port the Python code and make it more efficient is to think about the data involved and how that data needs to accessed. 

**** Data

- Body text. We will store this as a vector (array) of numbers representing the input text for training.
- Pair frequencies. We need to keep track of all the pairs in the body text and their frequencies. 

**** Access patterns

- Body text. We need sequential access to scan for pairs. The we need to be able to delete elements as part of the merge process.
- Pair frequencies. We need to be able to store the pairs and their frequencies and efficiently update them as we scan the body text. In addition we need fast access to the next most frequent pair.

**** Implementation

***** Body text

Because the body text required sequential access and the ability to quickly remove elements I used a singly linked list, or ~forward_list~. This has the desirable properties of sequential access and O(1) deletions.

~forward_list~ has the lowest memory overhead of all std C++ containers (a single pointer to the next element.

Other valid options considered:

1. Keep in a vector but use tombstones for removed items. This has the
   advantage of eliminating the memory moves for each replacement, and
   it doesn't have the problem forward list has with giving us a way
   to know the position in the input text (see later). This is quite a
   tricky implementation but perfectly feasible.
2. Keep in a vector and do the memory moves. Requires a lot of memory
   bandwidth and cpu for the copying but it is simple.

***** Pair frequencies

Ultimately I needed multiple structures here as I wanted to support more than one conflict resolution strategy and since these are picked by the user at runtime we need dynamic dispatch. So first I made a virtual class with the required interface for both:

#+BEGIN_SRC c++
template<typename T>
class PairCount {
public:
    // Virtual destructor to ensure proper cleanup of derived classes.
    virtual ~PairCount() = default;

    // Gets the total number of unique pairs stored.
    virtual size_t get_count() = 0;

    // Retrieves the count for a specific pair.
    virtual optional<int> get_pair(pair<T,T> mp) = 0;

    // Creates a new pair or modifies the frequency of an existing one.
    virtual bool create_or_modify_pair(T a, T b, int freq) = 0;

    // Gets the pair with the highest count.
    virtual optional<pair<T,T>> get_top_pair_count() = 0;

    // Retrieves all pairs and their counts.
    virtual std::vector<std::vector<T>> get_all() = 0;
};
#+END_SRC

Note that class has a template parameter, as the Tokenizer can be recompiled with different underlying numeric types for the tokens.

****** Conflict resolution strategy: First seen in input

Imagine a sequence as follows:

1,2,8,9,3,4...

After counting all the pairs we find that [1,2] and [3,4] have the same frequency.

a) [1,2] => 20 
b) [3,4] => 20

In this case we pick the one add first, which means the one first seen
in the input text.

#+begin_quote
In Python this insertion order comes for free because of Raymond
Hettinger's 2012 redesign of the Python dictionary. Implemented in
Python 3.6 (released December 23, 2016), introduced compact
dictionaries with key-sharing and faster performance. A side effect of
this redesign was that dictionaries began preserving insertion order
as an implementation detail. This was later formalized as a language
guarantee in Python 3.7 (released June 27, 2018), meaning dictionaries
officially maintain the order of key-value pairs as they are inserted.
#+end_quote

In Karpathy's code you can see that he simply relies on this behaviour
to get the consistent result based on above.

#+BEGIN_SRC python
# count up the number of times every consecutive pair appears
stats = get_stats(ids)
# find the pair with the highest count
pair = max(stats, key=stats.get)
#+END_SRC

And from the Python documentation: https://docs.python.org/3/library/functions.html#max

#+begin_quote
If multiple items are maximal, the function returns the first one
encountered. This is consistent with other sort-stability preserving
tools such as sorted(iterable, key=keyfunc, reverse=True)[0] and
heapq.nlargest(1, iterable, key=keyfunc).

In order to implement that we must track the insertion order. Rather
than let the user deal with that I built it into the PairCount
class. As elements are added, new ones get the current count and the
count is incremented.
#+end_quote

Picking a data structure here is tricky because we want to be able to
quickly store and modify pair frequencies (unordered_map), and a way
to get the most frequent (priority_queue). Furthermore, we want to
keep track of insertion order?

Sometimes you need to use multiple data structures to support a use case with conflicting requirements. For this purpose I used the ~boost::multi_index~.

https://www.boost.org/doc/libs/1_88_0/libs/multi_index/doc/index.html

There's nothing to stop you from using a set and a priority queue and
tracking them yourself, but multi_index handles that for you based on
the declaration of which indexes and access patterns you need.

Let's take a look at the implementation of ~PairCountInsertOrder~:

First the data; we need to store pair, the count and the insert order.

#+BEGIN_SRC c++
template<typename T>
struct PairCountOrder {
    ::pair<T,T> pair;
    int count;
    size_t insert_order;

    PairCountOrder(::pair<T,T> p, int c, size_t fo) : pair(p), count(c), insert_order(fo) {}
    PairCountOrder(::pair<T,T> p, int c) : pair(p), count(c), insert_order(std::numeric_limits<size_t>::max()) {}
};

// Comparison struct for sorting. Sorts by count (descending), then by insertion order (ascending).
template<typename T>
struct CompareCountOrder {
    bool operator()(const PairCountOrder<T>& a, const PairCountOrder<T>& b) const {
        if(a.count == b.count) {
            return a.insert_order < b.insert_order;
        } else {
            return a.count > b.count; // higher count is greater
        }
    }
};
#+END_SRC

Next we define the container itself. We just specify the indexes required and Boost takes care of picking the underlying data structures.

#+BEGIN_SRC c++
template<typename T>
using PairCountStore = boost::multi_index_container<
    PairCountOrder<T>,
    indexed_by<
        // Index 0: Hashed unique index on the 'pair' member for fast lookups.
        hashed_unique<member<PairCountOrder<T>, pair<T,T>, &PairCountOrder<T>::pair>>,
        // Index 1: Ordered non-unique index for sorting by count and insertion order.
        ordered_non_unique<identity<PairCountOrder<T>>, CompareCountOrder<T>>
    >
>;
#+END_SRC

Index 0 explanation:
It is hashed so we should get an O(1) lookup type, and unique meaning keys are unique, each pair can occur once only.
The rest of the declaration explains how to get the key for this index (use the pair member).

Index 1 explanation: 
This needs to be an ordered collection so we can extract the highest
frequency. It also needs to be non-unique (in its sort criteria),
because we can have multiple elements with the same frequency.

Now in our code we can grab the appropriate index depending on the
current purpose and when we make modifications to the data the boost
library will ensure the changes are synchronized across all the
indexes in the container.

#+BEGIN_SRC C++
auto& index_by_key = pcs.template get<0>();
auto f = index_by_key.find(mp);
if(f != pcs.end()) {
    index_by_key.modify(f, [freq](PairCountOrder<T>& pc) { pc.count += freq; });
    return false;
} else {
    pcs.insert(PairCountOrder<T>(mp, freq, next_insert++));
    return true;
}
#+END_SRC

****** Conflict resolution strategy: Lexicographical

Referred to as lexical in my implementation to save typing, this method means we pick from pairs based on which comes first. For example given the following two pairs:

a) [1,2] => 20 
b) [3,4] => 20

They have the same frequency so we pick a) as 1 < 3. The second member
of the pair is used as the tie-breaker, and of course if both members
are the same then they would be combined to a single entry in the
PairCount.

Again a multi_index container is needed here. Let's start with the data:

#+BEGIN_SRC c++
template<typename T>
struct PairCountLexical {
    ::pair<T,T> pair;
    int count;

    PairCountLexical(::pair<T,T> p, int c) : pair(p), count(c) {}
};

// Comparison struct for sorting. Sorts by count (descending), then by pair (lexical ascending).
template<typename T>
struct CompareLexicalOrder {
    bool operator()(const PairCountLexical<T>& a, const PairCountLexical<T>& b) const {
        if(a.count == b.count) {
            if (a.pair.first == b.pair.first) {
                return a.pair.second < b.pair.second;
            } else {
                return a.pair.first < b.pair.first;
            }
        } else {
            return a.count > b.count; // higher count is greater
        }
    }
};
#+END_SRC

And the container looks like this:

#+BEGIN_SRC c++
template<typename T>
using PairCountLexicalStore = boost::multi_index_container<
    PairCountLexical<T>,
    indexed_by<
        // Index 0: Hashed unique index on the 'pair' member for fast lookups.
        hashed_unique<member<PairCountLexical<T>, pair<T,T>, &PairCountLexical<T>::pair>>,
        // Index 1: Ordered non-unique index for sorting by count and lexical order.
        ordered_non_unique<identity<PairCountLexical<T>>, CompareLexicalOrder<T>>
    >
>;
#+END_SRC

Index 0 explanation:
Same as above this gives us fast insert, modify and lookup for the pair frequencies.

Index 1 explanation:
Same as above except the outcome is different because of the implementation of ~CompareLexicalOrder~.



** Conclusion
This project was a fantastic learning experience. It solidified my understanding of tokenization and was a practical lesson in the art of performance optimization. It demonstrates the enduring value of C++ for high-performance computing and highlights how modern tools like Zig can make the development experience much more pleasant.

If you want to dive into the code or run the benchmarks yourself, you can find the full project on GitHub.

- [[https://github.com/justinhj/minbpe-cc]]

** References



Thanks for reading!

\copy2025 Justin Heyes-Jones. All Rights Reserved
