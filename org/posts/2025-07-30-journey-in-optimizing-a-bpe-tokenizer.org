#+TITLE: Let's build the GPT Tokenizer in C++
#+AUTHOR: Justin Heyes-Jones
#+DATE: 2025-07-30
#+STARTUP: showall
#+OPTIONS: toc:nil
#+CREATOR: <a href="https://www.gnu.org/software/emacs/">Emacs</a> 29.1 (<a href="http://orgmode.org">Org</a> mode 9.6)
#+BEGIN_EXPORT html
---
layout: post
title: Let's build the GPT Tokenizer in C++
tags: [ai, cpp, c++, python, optimization, performance, llm, tokenizer]
---
<link rel="stylesheet" type="text/css" href="../../../_orgcss/site.css" />
#+END_EXPORT

** Introduction

In February 2024, reknowned AI researcher Andrej Karpathy published the following video. 

https://www.youtube.com/watch?v=zduSFxRajkE

An excellent introduction to the fascinating topic of tokenization,
something he considers a necessary evil right now. Maybe the need for
it will go away but for now it must be well understood by AI engineers
in order to get good results when building and using large language
models.

As a software engineer in 2025 LLM and generative AI in general brings
a profound shift into how I design, develop and implement systems. As
such I have been delving into the details of how they they work with
Tokenization being a significant side quest.

In this blog, related video and accompanying C++ project, I will walk
through my exploration of the training aspect of tokenizers and look
at their performance characteristics and optimization opportunities.

** Briefly, Tokenization and BPE

I don't want to even try and do a better job of explaining the need
for tokenization and the BPE algorithm, so I instead recommend
watching Karpathy's video and for additional insights read the paper
below:

[Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909)

*** The need for tokenization

- Large language models deal with numbers not words or characters, so we must map input text to numbers.
- The model requires a fixed size vocabulary.
- Unforunately that vocabulary would have to be enormous to fit all words from all languages.

*** Sub word unit tokenization

The solution to these issues is sub-word tokenization; splitting words
up into numbers representing components of words. Once you have that
you can handle:

- Open-Vocabulary: Part of the token set is the basic characters of each language so you can represent every word.
- Rare words: Because the vocabulary set is open it means any rare word is handled.
- Enables Translation of Novel Words: The model can translate and generate words it has not encountered before by composing them from sub-word units.

Tokenization means taking text and splitting it into subword units. An input text like ~"My cat, Blivarian, is making a mess."~ may be tokenized into something like this:

You can explore this tokenization here:
https://platform.openai.com/tokenizer

#+BEGIN_EXPORT html
<p style="text-xl">
  <span style="background-color: #d1c4e9;">My </span>
  <span style="background-color: #c8e6c9;">cat</span>
  <span style="background-color: #f0f4c3;">, </span>
  <span style="background-color: #ffcdd2;">Bl</span>
  <span style="background-color: #b3e5fc;">iv</span>
  <span style="background-color: #d1c4e9;">arian</span>
  <span style="background-color: #f0f4c3;">, </span>
  <span style="background-color: #c8e6c9;"> is</span>
  <span style="background-color: #ffcdd2;"> making</span>
  <span style="background-color: #b3e5fc;"> a</span>
  <span style="background-color: #d1c4e9;"> mess</span>
  <span style="background-color: #c8e6c9;">.</span>
</p>
#+END_EXPORT

[5444, 9059, 11, 3130, 569, 21203, 11, 382, 4137, 261, 13017, 13]

Notice that the commas have the same token value when appearing in
different places. Also that common words like cat and mess have their
own tokens.

I deliberately made up a name for the made up Cat that is not a real
word "Blivarian". You can see that it is split up into 3 sub
words. Without tokenization this would instead have been stored with a
special "Out of vocabulary" token, that means it carries no semantic
meaning. When dealing with sub word tokens however, the LLM has the
opportunity to build up meaning for those components that may help
with overall model quality.

*** Byte Pair Encoding - BPE

**** Why BPE?

From above we understand that we should split words into sub-word components to handle the vast space of human vocabulary in the finite space of the LLMs vocabulary.

How to do that is the next question. Why not, for example, just have a vocabulary consisting of the punctuation and alphabetic characters of every language?

It won't work well because in the LLM training it will build up an
embedding vector for each token, the unit of vocabulary. This vector
is an array of numbers that represents a direction in multidimensional
space. To us those numbers mean nothing, but in LLM training those
numbers, when used in conjunction with the rest of the models weights,
can be used to learn and represent all kinds of meaning.

Models like Transformers have a finite context window. When sequences
are excessively long, it becomes much harder for the model to capture
long-range dependencies and relationships between words that are far
apart. The model has to work harder to understand the overall context.

Instead we want something that splits things into meaningful chunks, morphemes, as well as capturing commonly used words with tokens. This ends up looking something like Huffman Encoding:

https://en.wikipedia.org/wiki/Huffman_coding

It represents more frequently occuring substrings with less bits, giving us a more efficicent data size.

Similarly, BPE, is data-driven algorithm that creates a vocabulary of meaningful and frequently occurring subword units.

**** BPE algorithm

First you need to train across a large corpus of realistic text. For
state of the art (SOTA) LLMs this is likely in the trillions of
characters of data.

The algorithm itself is very simple, it works as follows:

Start with 256 tokens (0 to 255), our basic character set.

1. First turn the text into its underlying numberic representation (utf-8 format input as bytes is fine).
2. Count all the pairs of bytes. 
3. Pick the most frequently occuring pair and generate the next new token (257, 258...).
4. Replace that pair whereever it occurs with the new token.

Repeat until you have your full vocabulary. You can then save the merge pairs and these are then used by end users to encode their text before sending to the model. 

They can also be used to reconstuct the original text in the decoding process when the response comes from the model. 

**** Conflict Resolution

An important decision in tokenization is how to handle pairs with the same frequency. In this post I'll consider two methods:

- First in corpus wins. 
- Lexicographical ordering.

With any tokenization algorithm design we need to consider efficiency of implementation alongside methods that give the best results.

** minbpe-cc. A rewrite of minbpe in modern C++.

It's a fair question. The Python code is clean, educational, and works perfectly. My motivation was twofold:

1.  *Self-Education*: The best way to learn something is to build it. Rewriting the code in a lower-level language like C++ would force me to confront every detail of the Byte-Pair Encoding (BPE) algorithm, from memory management to performance trade-offs.
2.  *The Craft of Optimization*: This is a classic software engineering exercise. Python is fantastic for prototyping and research, but for raw, number-crunching performance, we often turn to languages like C++. This project was a perfect real-world test case for that practice.

** The Heart of the Algorithm: Finding Frequent Pairs

The core loop of the BPE algorithm is conceptually simple: find the most frequent pair of adjacent tokens in the text and merge them into a new, single token. You repeat this process for a set number of merges to build your vocabulary.

The interesting part is /how/ you define "most frequent" when there are ties. This led to two different approaches in my C++ version.

*** The First Pair (Compatible Mode)
To maintain 1:1 compatibility with Karpathy's original output, the first approach is simple: if multiple pairs have the same highest frequency, you pick the one that appears *first* in the current vocabulary list. This is straightforward but requires more searching.

*** The Lexical Pair (Optimized Mode)
For maximum speed, I implemented a second strategy. When multiple pairs have the same highest frequency, you instead pick the pair that comes first *lexicographically*. This small change allows for significant optimization in the search process, as we'll see in the benchmarks.

** The Payoff: Performance Benchmarks

This is where the effort really shines. I ran the original Python code against my two C++ versions on two different datasets: the "Complete Works of Shakespeare" and a slice of the much larger "Wikitext-103" dataset.

The results are dramatic.

#+CAPTION: Tokenizer training time in seconds (lower is better).
| Implementation      | Dataset         | Time (seconds) |
|---------------------+-----------------+----------------|
| Python              | Shakespeare     |           15.5 |
| C++ (First Pair)    | Shakespeare     |           0.45 |
| C++ (Lexical Pair)  | Shakespeare     |           *0.12* |
|---------------------+-----------------+----------------|
| Python              | Wikitext-103    |          180.2 |
| C++ (First Pair)    | Wikitext-103    |            5.1 |
| C++ (Lexical Pair)  | Wikitext-103    |           *1.4* |

The C++ versions are orders of magnitude faster. The optimized "lexical pair" approach provides another 3-4x speedup over the already fast "compatible" C++ version.

** The C++ Struggle is Real

Of course, these performance gains didn't come for free. Writing modern C++ can be a journey, and I hit a few notable roadblocks.

*** Build Systems: CMake vs. Zig
I started with CMake, the de-facto standard for C++ projects. However, I quickly found myself wrestling with boilerplate. I eventually discovered that I could use the [[https://ziglang.org/][Zig]] toolchain not just for Zig code, but as an incredibly clean and simple C++ build system.

Compare the verbosity of a typical ~CMakeLists.txt~ to my final ~build.zig~ file:
#+BEGIN_SRC zig
const std = @import("std");

pub fn build(b: *std.Build) void {
    const target = b.standardTargetOptions(.{});
    const optimize = b.standardOptimizeOption(.{});

    const exe = b.addExecutable(.{
        .name = "minbpe",
        .root_source_file = .{ .path = "main.cpp" },
        .target = target,
        .optimize = optimize,
    });
    // ... link libraries
    b.installArtifact(exe);
}
#+END_SRC

This was a revelation. It's cross-platform and ridiculously easy to use.

*** The Regex Nightmare
The GPT-4 tokenizer pattern uses a tricky bit of regex: a *negative lookahead*. Finding a modern, header-only C++ regex library with good Unicode support that could handle this correctly was surprisingly difficult.

*** A Subtle Bug: Integer Sizes
I originally used a standard C++ ~int~ for token IDs. On my 64-bit system, this was overkill and could mask potential bugs. A ~uint16_t~ (max value 65,535) is a common choice, but to support vocabularies up to 100k, I settled on ~uint32_t~. It's a small detail, but it's the kind of memory and type consideration that is front-and-center in C++ development.

** Conclusion
This project was a fantastic learning experience. It solidified my understanding of tokenization and was a practical lesson in the art of performance optimization. It demonstrates the enduring value of C++ for high-performance computing and highlights how modern tools like Zig can make the development experience much more pleasant.

If you want to dive into the code or run the benchmarks yourself, you can find the full project on GitHub.

- [[https://github.com/justinhj/minbpe-cc]]

Thanks for reading!

\copy2025 Justin Heyes-Jones. All Rights Reserved
